# HOME UI + LLM Config/Status

## Descripci贸n

La nueva interfaz HOME (`/`) proporciona un dashboard centralizado para CometLocal con:

- **Navegaci贸n completa**: Accesos directos a todas las pantallas del sistema
- **Configuraci贸n LLM persistente**: Gesti贸n de servidor, modelo y par谩metros
- **Monitor de estado LLM**: Verificaci贸n en tiempo real del estado del servidor externo

## Rutas Implementadas

### Frontend
- `GET /` - **HOME Dashboard** (nueva p谩gina principal)
- `GET /home` - Alias para HOME Dashboard
- `GET /index.html` - Chat UI (legacy)

### Backend APIs
- `GET /api/config/llm` - Obtiene configuraci贸n LLM actual
- `POST /api/config/llm` - Actualiza configuraci贸n LLM (persiste en disco)
- `GET /api/health/llm` - Verifica estado del servidor LLM externo

## Funcionalidades

### 1. Quick Links (Accesos R谩pidos)
Navegaci贸n a todas las pantallas existentes:
-  Chat Principal (`/index.html`)
-  Form Sandbox (`/form-sandbox`)
-  CAE Training (`/training`)
-  Portal CAE A (`/simulation/portal_a/login.html`)
-  Portal CAE A v2 (`/simulation/portal_a_v2/login.html`)
-  Runs Viewer (`/runs`)
- 锔 Configuraci贸n (`/config`)
-  API Docs (`/docs`)
- わ Health Check (`/health`)

### 2. Configuraci贸n LLM
Campos editables con persistencia:
- **Base URL**: URL del servidor LLM (ej: `http://127.0.0.1:1234/v1`)
- **API Key**: Clave de autenticaci贸n
- **Proveedor**: lm-studio, openai, anthropic, ollama
- **Modelo**: Nombre/identificador del modelo
- **Timeout**: Segundos de espera (1-300)

**Persistencia**: Se guarda autom谩ticamente en `data/refs/llm_config.json`

### 3. Monitor de Estado LLM
Verificaci贸n autom谩tica cada 30 segundos:
- **Estado**:  Online /  Degradado /  Offline
- **Latencia**: Tiempo de respuesta en ms
- **ltimo check**: Timestamp del 煤ltimo chequeo
- **Servidor**: URL del servidor verificado
- **Bot贸n manual**: "Verificar Ahora"

## Configuraci贸n T茅cnica

### Archivos
- `frontend/home.html` - UI principal del dashboard
- `backend/app.py` - APIs y rutas backend
- `backend/config.py` - Configuraci贸n LLM persistente
- `data/refs/llm_config.json` - Archivo de configuraci贸n persistente

### Dependencias
- `aiohttp` - Para health checks HTTP
- `openai` - Cliente LLM (ya existente)

## C贸mo Probar

### 1. Levantar Backend
```bash
cd CometLocal
python -m uvicorn backend.app:app --host 127.0.0.1 --port 8000
```

### 2. Acceder al Dashboard
- Abrir `http://127.0.0.1:8000/` en navegador
- Verificar que se muestran todos los Quick Links
- Verificar que se carga la configuraci贸n LLM

### 3. Probar Configuraci贸n
- Cambiar Base URL a `http://127.0.0.1:9999/v1` (puerto inexistente)
- Guardar configuraci贸n
- Verificar que el estado cambia a  Offline
- Reiniciar backend y verificar que la configuraci贸n persiste

### 4. Probar Estado LLM
- Configurar URL v谩lida de servidor LLM (ej: LM Studio en puerto 1234)
- Verificar que el estado muestra  Online con latencia
- Usar bot贸n "Verificar Ahora" para actualizaci贸n manual

## Estados del Monitor LLM

### Online ()
- Respuesta HTTP 200 del endpoint `/models`
- Se muestra latencia en ms
- Estado: "online"

### Degradado ()
- Respuesta HTTP no 200 pero v谩lida
- Se muestra c贸digo de error HTTP
- Estado: "degraded"

### Offline ()
- Timeout o error de conexi贸n
- Se muestra detalle del error
- Estado: "offline"

## API Reference

### GET /api/config/llm
**Respuesta:**
```json
{
  "base_url": "http://127.0.0.1:1234/v1",
  "api_key": "lm-studio",
  "provider": "lm-studio",
  "model": "local-model",
  "timeout_seconds": 30
}
```

### POST /api/config/llm
**Cuerpo:**
```json
{
  "base_url": "http://127.0.0.1:1234/v1",
  "api_key": "your-key",
  "provider": "lm-studio",
  "model": "model-name",
  "timeout_seconds": 30
}
```

**Respuesta:**
```json
{
  "status": "ok",
  "message": "LLM config updated successfully"
}
```

### GET /api/health/llm
**Respuesta:**
```json
{
  "ok": true,
  "latency_ms": 45,
  "base_url": "http://127.0.0.1:1234/v1",
  "status": "online"
}
```

O cuando offline:
```json
{
  "ok": false,
  "latency_ms": null,
  "base_url": "http://127.0.0.1:9999/v1",
  "status": "offline",
  "detail": "Timeout"
}
```

## Notas de Implementaci贸n

- **Thread Safety**: El health check usa aiohttp con timeout corto para evitar bloquear el servidor
- **Persistencia**: Configuraci贸n se guarda at贸micamente en JSON
- **Fallback**: Si no hay configuraci贸n, usa valores por defecto
- **Auto-refresh**: UI actualiza estado cada 30 segundos autom谩ticamente
- **Validaci贸n**: Campos requeridos se validan antes de guardar

## Troubleshooting

### Problema: APIs devuelven "Not Found"
- Verificar que el backend se reinici贸 completamente
- Revisar logs del servidor por errores de importaci贸n

### Problema: Configuraci贸n no persiste
- Verificar permisos de escritura en `data/refs/`
- Revisar que `llm_config.json` se crea correctamente

### Problema: Health check siempre offline
- Verificar que el servidor LLM est茅 ejecut谩ndose
- Probar conectividad manual: `curl http://127.0.0.1:1234/v1/models`
- Revisar configuraci贸n de firewall/antivirus
